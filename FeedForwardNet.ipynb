{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStepFunction(gdVal):\n",
    "    if gdVal > 0 :\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_RELU(x):\n",
    "    if x < 0 : \n",
    "        return x\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x-x.max())/ np.sum(np.exp(x-x.max()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    if (a>= 0):\n",
    "        return 1 / (1+math.exp(-a))\n",
    "    else:\n",
    "        return math.exp(a) / (1+math.exp(a))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_fct(a):\n",
    "    #t = (2 / (1 + math.exp((-2)*a))) - 1\n",
    "    t = np.tanh(a)\n",
    "    # both return same value\n",
    "    return t\n",
    "    #dt=1-t**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    \n",
    "    X=[x[0],x[1],1]\n",
    "    \n",
    "    XW = np.zeros((3,2))\n",
    "    XW[0][0] = 0.10\n",
    "    XW[0][1] = 0.15\n",
    "    XW[1][0] = 0.20\n",
    "    XW[1][1] = 0.25\n",
    "    XW[2][0] = 0.30\n",
    "    XW[2][1] = 0.30\n",
    "    print(\"Weights layer 1: \",XW)\n",
    "\n",
    "    #in_h = [0,0]\n",
    "    in_h = np.dot(X,XW)\n",
    "    print(\"Input to Hidden Layer: \", in_h)\n",
    "\n",
    "    out_h = np.zeros((1,3))\n",
    "    out_h = [sigmoid(in_h[0]),sigmoid(in_h[1]),1]\n",
    "    print(\"Output From Hidden Layer: \",np.round(out_h,2))\n",
    "\n",
    "    ZW = np.zeros((3,2))\n",
    "    ZW[0][0] = 0.35\n",
    "    ZW[0][1] = 0.40\n",
    "    ZW[1][0] = 0.45\n",
    "    ZW[1][1] = 0.50\n",
    "    ZW[2][0] = 0.55\n",
    "    ZW[2][1] = 0.55\n",
    "    in_O = np.dot(out_h,ZW)\n",
    "\n",
    "    Y = np.round([sigmoid(in_O[0]), sigmoid(in_O[1])],2)\n",
    "    print(\"The expected outputs of the network are:\", Y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(y,y_hat):\n",
    "    error = 0\n",
    "    for i in range(len(y)):\n",
    "        error += 1/2*((y[i] - y_hat[i])**2) #MSRE\n",
    "        #error += y[i]*np.log(y_hat[i]) #cross entropy\n",
    "        #error += -(y[i]*np.log(y_hat[i]) - (1-y[i])*np.log(1-y_hat[i])) # binary cross entropy\n",
    "    error = np.round(error,2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights layer 1:  [[0.1  0.15]\n",
      " [0.2  0.25]\n",
      " [0.3  0.3 ]]\n",
      "Input to Hidden Layer:  [0.35  0.365]\n",
      "Output From Hidden Layer:  [0.59 0.59 1.  ]\n",
      "The expected outputs of the network are: [0.74 0.75]\n",
      "The error is:  22.0 %\n"
     ]
    }
   ],
   "source": [
    "y = [0.10,0.90]\n",
    "y_hat = forward_pass(x = [0.10,0.20])\n",
    "error = get_error(y,y_hat)\n",
    "print(\"The error is: \",error*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algo(x):\n",
    "    eta = 0.01\n",
    "    y = [0.10,0.90]\n",
    "    X=[x[0],x[1],1]\n",
    "    \n",
    "    XW = np.zeros((3,2))\n",
    "    XW[0][0] = 0.10\n",
    "    XW[0][1] = 0.15\n",
    "    XW[1][0] = 0.20\n",
    "    XW[1][1] = 0.25\n",
    "    XW[2][0] = 0.30\n",
    "    XW[2][1] = 0.30\n",
    "    print(\"Weights layer 1: \",XW)\n",
    "\n",
    "    ZW = np.zeros((3,2))\n",
    "    ZW[0][0] = 0.35\n",
    "    ZW[0][1] = 0.40\n",
    "    ZW[1][0] = 0.45\n",
    "    ZW[1][1] = 0.50\n",
    "    ZW[2][0] = 0.55\n",
    "    ZW[2][1] = 0.55\n",
    "    print(\"Weights layer 2: \",ZW)\n",
    "    \n",
    "    error = 100\n",
    "    delta_ZW = np.zeros((3,2))\n",
    "    delta_XW = np.zeros((3,2))\n",
    "    \n",
    "    while error > 0.0000001:\n",
    "        #in_h = [0,0]\n",
    "        in_h = np.dot(X,XW)\n",
    "        #print(\"Input to Hidden Layer: \", in_h)\n",
    "\n",
    "        out_h = np.zeros((1,3))\n",
    "        out_h = [sigmoid(in_h[0]),sigmoid(in_h[1]),1]\n",
    "        #print(\"Output From Hidden Layer: \",np.round(out_h,2))\n",
    "\n",
    "        in_O = np.dot(out_h,ZW)\n",
    "\n",
    "        Y = np.round([sigmoid(in_O[0]), sigmoid(in_O[1])],2)\n",
    "        #print(\"The expected outputs of the network are:\", Y)\n",
    "        \n",
    "        for t in range(0,len(X)):\n",
    "            for i in range(2):\n",
    "                # error = desired - actuak\n",
    "                # update = learningDactor * (desired - actual).input\n",
    "                # i is the column\n",
    "                delta_ZW[t,i] = eta * (y[i]-Y[i]) * out_h[i] \n",
    "            \n",
    "        for t in range(0,len(X)):\n",
    "            for i in range(2):\n",
    "                # error = desired - actuak\n",
    "                # update = learningDactor * (desired - actual).input\n",
    "                #print(1-out_h[i])\n",
    "                # error changes with gradient since this is a non linear regression use this below with MRSE\n",
    "                # output is not class 0 or 1\n",
    "                delta_XW[t,i] = eta * (y[i]-Y[i]) * delta_ZW[t,i] * out_h[i]*(1-out_h[i])*X[t] \n",
    "        ZW = ZW + delta_ZW\n",
    "        XW = XW + delta_XW\n",
    "        \n",
    "        error = get_error(y,Y)\n",
    "        \n",
    "    print(\"Error after backpropagation: \",error)\n",
    "    print(\"New predicted output: \", Y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights layer 1:  [[0.1  0.15]\n",
      " [0.2  0.25]\n",
      " [0.3  0.3 ]]\n",
      "Weights layer 2:  [[0.35 0.4 ]\n",
      " [0.45 0.5 ]\n",
      " [0.55 0.55]]\n",
      "Error after backpropagation:  0.0\n",
      "New predicted output:  [0.19 0.86]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.19, 0.86])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_algo(x = [0.10,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
